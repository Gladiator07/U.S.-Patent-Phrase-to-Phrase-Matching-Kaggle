# https://huggingface.co/microsoft/deberta-large
# Configuration for A100
output_dir: ${paths.out_dir}

# evaluation, logging & saving
evaluation_strategy: steps
logging_strategy: steps
save_strategy: steps
save_total_limit: 1
report_to: wandb
save_steps: 250
eval_steps: 250
logging_steps: 250

load_best_model_at_end: True
metric_for_best_model: eval_pearsonr
greater_is_better: True

# optimization
learning_rate: 2.0e-5
warmup_ratio: 0.1
weight_decay: 0.01
lr_scheduler_type: cosine
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-6

# train settings
num_train_epochs: 5
gradient_accumulation_steps: 1
per_device_train_batch_size: 64
per_device_eval_batch_size: 96
group_by_length: True
fp16: True

# Misc
remove_unused_columns: True
dataloader_pin_memory: True
dataloader_num_workers: 2
seed: 42
